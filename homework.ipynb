{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "\n",
    "\n",
    "file_path = './amazon_reviews_us_Office_Products_v1_00.tsv.gz'\n",
    "\n",
    "\n",
    "df = pd.read_csv(gzip.open(file_path), sep='\\t', usecols=['review_body', 'star_rating'])\n",
    "\n",
    "df = df[pd.to_numeric(df['star_rating'], errors='coerce').notna()]\n",
    "\n",
    "df['star_rating'] = df['star_rating'].astype(int)\n",
    "df = df.dropna()\n",
    "\n",
    "rating_dfs = []\n",
    "for i in range(1, 6):\n",
    "    rating_df = df[df['star_rating'] == i].sample(n=50000, random_state=42)\n",
    "    rating_dfs.append(rating_df)\n",
    "\n",
    "dataset = pd.concat(rating_dfs, ignore_index=True)\n",
    "\n",
    "def categorize_rating(rating):\n",
    "    if rating > 3:\n",
    "        return 1\n",
    "    elif rating < 3:\n",
    "        return 2\n",
    "    else:  \n",
    "        return 3\n",
    "    \n",
    "\n",
    "dataset['class'] = dataset['star_rating'].apply(categorize_rating)\n",
    "\n",
    "dataset.to_csv('data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7118192911148071)]\n"
     ]
    }
   ],
   "source": [
    "# Perform the vector arithmetic indirectly using `most_similar`\n",
    "result = wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'excellent'\t'outstanding'\t0.56\n"
     ]
    }
   ],
   "source": [
    "w1='excellent'\n",
    "w2='outstanding'\n",
    "print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Trained Model Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The photo is deceiving - makes it look like a ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Worst labels ever! I purchased these labels to...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>This product broke in a very short time.  It a...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>The printer head is malfunctioning since the i...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>When this item shipped to me I was very excite...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249995</th>\n",
       "      <td>5</td>\n",
       "      <td>Produces great prints.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249996</th>\n",
       "      <td>5</td>\n",
       "      <td>perfect for my high school student to use in h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249997</th>\n",
       "      <td>5</td>\n",
       "      <td>The product was Excellent! !</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249998</th>\n",
       "      <td>5</td>\n",
       "      <td>Arrived fast and works great--good buy!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249999</th>\n",
       "      <td>5</td>\n",
       "      <td>I am glad I bought these headsets. I can hear ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        star_rating                                        review_body  class\n",
       "0                 1  The photo is deceiving - makes it look like a ...      2\n",
       "1                 1  Worst labels ever! I purchased these labels to...      2\n",
       "2                 1  This product broke in a very short time.  It a...      2\n",
       "3                 1  The printer head is malfunctioning since the i...      2\n",
       "4                 1  When this item shipped to me I was very excite...      2\n",
       "...             ...                                                ...    ...\n",
       "249995            5                             Produces great prints.      1\n",
       "249996            5  perfect for my high school student to use in h...      1\n",
       "249997            5                       The product was Excellent! !      1\n",
       "249998            5            Arrived fast and works great--good buy!      1\n",
       "249999            5  I am glad I bought these headsets. I can hear ...      1\n",
       "\n",
       "[250000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/64/glschqjx3sn28wyvf5wwcx2r0000gn/T/ipykernel_1842/1517175790.py:5: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  df['review_body'] = df['review_body'].apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n",
      "/var/folders/64/glschqjx3sn28wyvf5wwcx2r0000gn/T/ipykernel_1842/1517175790.py:5: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  df['review_body'] = df['review_body'].apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "df['review_body'] = df['review_body'].str.lower()\n",
    "\n",
    "df['review_body'] = df['review_body'].apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n",
    "df['review_body'] = df['review_body'].apply(lambda x: re.sub(r'http\\S+', ' ', x))\n",
    "df['review_body'] = df['review_body'].str.replace('[^a-zA-Z\\s]', ' ', regex=True)\n",
    "df['review_body'] = df['review_body'].str.replace('\\s+', ' ', regex=True)\n",
    "\n",
    "\n",
    "import contractions\n",
    "df['review_body'] = df['review_body'].apply(contractions.fix)\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "def lemmatize_text(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "def process_text(text):\n",
    "    # Tokenization\n",
    "    tokens = tokenize_text(text)\n",
    "    # Remove stop words\n",
    "    tokens_no_stopwords = remove_stopwords(tokens)\n",
    "    # Lemmatization\n",
    "    lemmatized_tokens = lemmatize_text(tokens_no_stopwords)\n",
    "\n",
    "    return lemmatized_tokens\n",
    "\n",
    "\n",
    "tokenized_data = df['review_body'].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [photo, deceiving, make, look, like, set, pen,...\n",
       "1         [worst, label, ever, purchased, label, try, re...\n",
       "2         [product, broke, short, time, also, poor, job,...\n",
       "3         [printer, head, malfunctioning, since, install...\n",
       "4         [item, shipped, excited, outside, great, quali...\n",
       "                                ...                        \n",
       "249995                              [produce, great, print]\n",
       "249996    [perfect, high, school, student, use, math, cl...\n",
       "249997                                 [product, excellent]\n",
       "249998              [arrived, fast, work, great, good, buy]\n",
       "249999    [glad, bought, headset, hear, better, ever, un...\n",
       "Name: review_body, Length: 250000, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(tokenized_data, vector_size=300, window=11, min_count=10)\n",
    "\n",
    "\n",
    "model.save(\"word2vec.model\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# Load model\n",
    "model = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5609798\n"
     ]
    }
   ],
   "source": [
    "similarity = model.wv.similarity('happy', 'impressed')\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'excellent'\t'outstanding'\t0.78\n"
     ]
    }
   ],
   "source": [
    "w1='excellent'\n",
    "w2='outstanding'\n",
    "print('%r\\t%r\\t%.2f' % (w1, w2, model.wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Word Pair  Pretrained Model Similarity  \\\n",
      "0    smartphone, camera                         0.32   \n",
      "1       laptop, charger                         0.47   \n",
      "2  headphone, bluetooth                         0.49   \n",
      "3         novel, author                         0.46   \n",
      "4    fiction, character                         0.25   \n",
      "\n",
      "   Amazon Review Model Similarity  \n",
      "0                            0.41  \n",
      "1                            0.25  \n",
      "2                            0.45  \n",
      "3                            0.58  \n",
      "4                            0.31  \n"
     ]
    }
   ],
   "source": [
    "word_pairs = [\n",
    "    ('smartphone', 'camera'), ('laptop', 'charger'), ('headphone', 'bluetooth'),\n",
    "    ('novel', 'author'), ('fiction', 'character'),\n",
    "]\n",
    "\n",
    "\n",
    "def compare_similarities(model1, model2, word_pairs):\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for word1, word2 in word_pairs:\n",
    "        similarity_model1 = model1.similarity(word1, word2)\n",
    "        similarity_model2 = model2.similarity(word1, word2)\n",
    "        results.append({\n",
    "            'Word Pair': f'{word1}, {word2}',\n",
    "            'Pretrained Model Similarity': round(similarity_model1, 2),\n",
    "            'Amazon Review Model Similarity': round(similarity_model2, 2)\n",
    "        })\n",
    "    \n",
    "   \n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "   \n",
    "    print(results_df)\n",
    "\n",
    "\n",
    "compare_similarities(wv, model.wv, word_pairs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron and SVM with Custom Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def average_word2vec(reviews, word2vec_model, vector_size):\n",
    "    features = []\n",
    "\n",
    "    for review in reviews:\n",
    "        \n",
    "        valid_words = [word for word in review if word in word2vec_model.wv.key_to_index]\n",
    "\n",
    "        if not valid_words:\n",
    "           \n",
    "            features.append(np.zeros(vector_size))\n",
    "            continue\n",
    "\n",
    "        \n",
    "        word_vectors = np.array([word2vec_model.wv[word] for word in valid_words])\n",
    "        avg_vector = word_vectors.mean(axis=0)\n",
    "        features.append(avg_vector)\n",
    "\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "avg_features = average_word2vec(tokenized_data, model, vector_size=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered DataFrame shape: (200000, 3)\n",
      "Filtered avg_features shape: (200000, 300)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_filtered = df[df['class'] != 3]\n",
    "\n",
    "filtered_indices = df_filtered.index.to_numpy()\n",
    "\n",
    "avg_features_filtered = avg_features[filtered_indices]\n",
    "\n",
    "\n",
    "print(\"Filtered DataFrame shape:\", df_filtered.shape)\n",
    "print(\"Filtered avg_features shape:\", avg_features_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=avg_features_filtered\n",
    "y=df_filtered['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200000, 300), (200000,))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics For Perceptron:\n",
      "Accuracy: 0.80206875\n",
      "Precision: 0.7983036618188103\n",
      "Recall: 0.8083332291575512\n",
      "F1-Score: 0.8032871402749222\n",
      "Testing Metrics For Perceptron:\n",
      "Accuracy: 0.803375\n",
      "Precision: 0.7995657751899734\n",
      "Recall: 0.8099165292147749\n",
      "F1-Score: 0.8047078687954708\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "perceptron_model = Perceptron()\n",
    "\n",
    "perceptron_model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = perceptron_model.predict(X_train)\n",
    "y_test_pred = perceptron_model.predict(X_test)\n",
    "\n",
    "\n",
    "accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "precision_train = precision_score(y_train, y_train_pred)\n",
    "recall_train = recall_score(y_train, y_train_pred)\n",
    "f1_train = f1_score(y_train, y_train_pred)\n",
    "accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "precision_test = precision_score(y_test, y_test_pred)\n",
    "recall_test = recall_score(y_test, y_test_pred)\n",
    "f1_test = f1_score(y_test, y_test_pred)\n",
    "\n",
    "\n",
    "print(\"\\nTraining Metrics For Perceptron:\")\n",
    "print(f\"Accuracy: {accuracy_train}\")\n",
    "print(f\"Precision: {precision_train}\")\n",
    "print(f\"Recall: {recall_train}\")\n",
    "print(f\"F1-Score: {f1_train}\")\n",
    "print(\"Testing Metrics For Perceptron:\")\n",
    "print(f\"Accuracy: {accuracy_test}\")\n",
    "print(f\"Precision: {precision_test}\")\n",
    "print(f\"Recall: {recall_test}\")\n",
    "print(f\"F1-Score: {f1_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/darshanrao/anaconda3/lib/python3.11/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/Users/darshanrao/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics For SVM:\n",
      "Accuracy: 0.84153125\n",
      "Precision: 0.8524577473874339\n",
      "Recall: 0.8259972747615416\n",
      "F1-Score: 0.8390189393217906\n",
      "Testing Metrics For SVM:\n",
      "Accuracy: 0.84235\n",
      "Precision: 0.8529184483025088\n",
      "Recall: 0.8275103713700205\n",
      "F1-Score: 0.8400223248262214\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm_model = LinearSVC()\n",
    "\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = svm_model.predict(X_train)\n",
    "y_test_pred = svm_model.predict(X_test)\n",
    "\n",
    "accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "precision_train = precision_score(y_train, y_train_pred)\n",
    "recall_train = recall_score(y_train, y_train_pred)\n",
    "f1_train = f1_score(y_train, y_train_pred)\n",
    "\n",
    "# Evaluate the model on testing data\n",
    "accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "precision_test = precision_score(y_test, y_test_pred)\n",
    "recall_test = recall_score(y_test, y_test_pred)\n",
    "f1_test = f1_score(y_test, y_test_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nTraining Metrics For SVM:\")\n",
    "print(f\"Accuracy: {accuracy_train}\")\n",
    "print(f\"Precision: {precision_train}\")\n",
    "print(f\"Recall: {recall_train}\")\n",
    "print(f\"F1-Score: {f1_train}\")\n",
    "print(\"Testing Metrics For SVM:\")\n",
    "print(f\"Accuracy: {accuracy_test}\")\n",
    "print(f\"Precision: {precision_test}\")\n",
    "print(f\"Recall: {recall_test}\")\n",
    "print(f\"F1-Score: {f1_test}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron and SVM with Google Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word2vec_google(reviews,vector_size):\n",
    "    features = []\n",
    "\n",
    "    for review in reviews:\n",
    "        \n",
    "        valid_words = [word for word in review if word in wv.key_to_index]\n",
    "\n",
    "        if not valid_words:\n",
    "           \n",
    "            features.append(np.zeros(vector_size))\n",
    "            continue\n",
    "\n",
    "        \n",
    "        word_vectors = np.array([wv[word] for word in valid_words])\n",
    "        avg_vector = word_vectors.mean(axis=0)\n",
    "        features.append(avg_vector)\n",
    "\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "avg_features_pretrained = average_word2vec_google(tokenized_data,vector_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered DataFrame shape: (200000, 3)\n",
      "Filtered avg_features shape: (200000, 300)\n"
     ]
    }
   ],
   "source": [
    "# Get the indices of the remaining rows after filtering\n",
    "filtered_indices = df_filtered.index.to_numpy()\n",
    "\n",
    "# Now, use these indices to filter the avg_features array\n",
    "avg_features_filtered_pretrained = avg_features_pretrained[filtered_indices]\n",
    "\n",
    "# Checking the dimensions to ensure they match\n",
    "print(\"Filtered DataFrame shape:\", df_filtered.shape)\n",
    "print(\"Filtered avg_features shape:\", avg_features_filtered_pretrained.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=avg_features_filtered_pretrained\n",
    "y=df_filtered['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics For Perceptron:\n",
      "Accuracy: 0.7436875\n",
      "Precision: 0.8928074807037343\n",
      "Recall: 0.5538234595527108\n",
      "F1-Score: 0.6835989939358403\n",
      "Testing Metrics For Perceptron:\n",
      "Accuracy: 0.7438\n",
      "Precision: 0.8917074737095609\n",
      "Recall: 0.5552056780126956\n",
      "F1-Score: 0.6843272548053229\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "perceptron_model = Perceptron()\n",
    "\n",
    "perceptron_model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = perceptron_model.predict(X_train)\n",
    "y_test_pred = perceptron_model.predict(X_test)\n",
    "\n",
    "\n",
    "accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "precision_train = precision_score(y_train, y_train_pred)\n",
    "recall_train = recall_score(y_train, y_train_pred)\n",
    "f1_train = f1_score(y_train, y_train_pred)\n",
    "accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "precision_test = precision_score(y_test, y_test_pred)\n",
    "recall_test = recall_score(y_test, y_test_pred)\n",
    "f1_test = f1_score(y_test, y_test_pred)\n",
    "\n",
    "\n",
    "print(\"\\nTraining Metrics For Perceptron:\")\n",
    "print(f\"Accuracy: {accuracy_train}\")\n",
    "print(f\"Precision: {precision_train}\")\n",
    "print(f\"Recall: {recall_train}\")\n",
    "print(f\"F1-Score: {f1_train}\")\n",
    "print(\"Testing Metrics For Perceptron:\")\n",
    "print(f\"Accuracy: {accuracy_test}\")\n",
    "print(f\"Precision: {precision_test}\")\n",
    "print(f\"Recall: {recall_test}\")\n",
    "print(f\"F1-Score: {f1_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/darshanrao/anaconda3/lib/python3.11/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics For SVM:\n",
      "Accuracy: 0.81735625\n",
      "Precision: 0.8353191376941773\n",
      "Recall: 0.7905316715212581\n",
      "F1-Score: 0.8123085223222029\n",
      "Testing Metrics For SVM:\n",
      "Accuracy: 0.816875\n",
      "Precision: 0.8335963804713805\n",
      "Recall: 0.7919728095166692\n",
      "F1-Score: 0.8122516980648469\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm_model = LinearSVC()\n",
    "\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = svm_model.predict(X_train)\n",
    "y_test_pred = svm_model.predict(X_test)\n",
    "\n",
    "accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "precision_train = precision_score(y_train, y_train_pred)\n",
    "recall_train = recall_score(y_train, y_train_pred)\n",
    "f1_train = f1_score(y_train, y_train_pred)\n",
    "\n",
    "# Evaluate the model on testing data\n",
    "accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "precision_test = precision_score(y_test, y_test_pred)\n",
    "recall_test = recall_score(y_test, y_test_pred)\n",
    "f1_test = f1_score(y_test, y_test_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nTraining Metrics For SVM:\")\n",
    "print(f\"Accuracy: {accuracy_train}\")\n",
    "print(f\"Precision: {precision_train}\")\n",
    "print(f\"Recall: {recall_train}\")\n",
    "print(f\"F1-Score: {f1_train}\")\n",
    "print(\"Testing Metrics For SVM:\")\n",
    "print(f\"Accuracy: {accuracy_test}\")\n",
    "print(f\"Precision: {precision_test}\")\n",
    "print(f\"Recall: {recall_test}\")\n",
    "print(f\"F1-Score: {f1_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Neural Networks Average Values Custom Word2Vec Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=avg_features_filtered\n",
    "y=df_filtered['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/darshanrao/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Validation Loss: 0.0054, Accuracy: 85.55%\n",
      "Epoch: 2, Validation Loss: 0.0052, Accuracy: 85.98%\n",
      "Epoch: 3, Validation Loss: 0.0052, Accuracy: 85.82%\n",
      "Epoch: 4, Validation Loss: 0.0051, Accuracy: 86.12%\n",
      "Epoch: 5, Validation Loss: 0.0051, Accuracy: 86.17%\n",
      "Epoch: 6, Validation Loss: 0.0051, Accuracy: 86.22%\n",
      "Epoch: 7, Validation Loss: 0.0051, Accuracy: 86.22%\n",
      "Epoch: 8, Validation Loss: 0.0050, Accuracy: 86.59%\n",
      "Epoch: 9, Validation Loss: 0.0050, Accuracy: 86.60%\n",
      "Epoch: 10, Validation Loss: 0.0050, Accuracy: 86.58%\n",
      "Epoch: 11, Validation Loss: 0.0050, Accuracy: 86.53%\n",
      "Epoch: 12, Validation Loss: 0.0050, Accuracy: 86.57%\n",
      "Epoch: 13, Validation Loss: 0.0050, Accuracy: 86.56%\n",
      "Epoch: 14, Validation Loss: 0.0050, Accuracy: 86.54%\n",
      "Early stopping triggered. Training stopped.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "y_train_tensor = y_train_tensor - 1\n",
    "y_val_tensor = y_val_tensor - 1\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define the MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(300, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "mlp_model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mlp_model.parameters(), lr=0.001)\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, epochs=10, patience=3):\n",
    "    # Initialize early stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "    \n",
    "    # Scheduler for learning rate decay\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=patience // 2, factor=0.1, verbose=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad() # clears old gradients,\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target) # Calculate the pred-actual Loss\n",
    "            loss.backward() # Back proprogation (Calculating the gradient)\n",
    "            optimizer.step() # Weight updates\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval() # Switches the into evaluation mode\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad(): # Ensures gradient is not calculated(Saves memory and computation)\n",
    "            for data, target in val_loader:\n",
    "                output = model(data) ## Output in the form of probabitilities\n",
    "                val_loss += criterion(output, target).item() \n",
    "                pred = output.argmax(dim=1, keepdim=True) # Probabitlity with the max value is the output\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item() # counts the number of correct predictions\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        accuracy = 100. * correct / len(val_loader.dataset)\n",
    "        print(f'Epoch: {epoch+1}, Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "        \n",
    "        # Early stopping logic\n",
    "        if val_loss < best_val_loss: # if the  current loss is less the best loss\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0 \n",
    "        else: # no improvement in loss\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print('Early stopping triggered. Training stopped.')\n",
    "                early_stop = True\n",
    "                break\n",
    "        \n",
    "        # Learning rate scheduler step\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if early_stop:\n",
    "            print(\"Stopped early at epoch:\", epoch+1)\n",
    "            break\n",
    "\n",
    "\n",
    "train_model(mlp_model, criterion, optimizer, train_loader, val_loader, epochs=100, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Metrics For MLP:\n",
      "Accuracy: 0.8796\n",
      "Precision: 0.8803\n",
      "Recall: 0.8787\n",
      "F1-Score: 0.8795\n",
      "\n",
      "Testing Metrics For MLP:\n",
      "Accuracy: 0.8654\n",
      "Precision: 0.8664\n",
      "Recall: 0.8640\n",
      "F1-Score: 0.8652\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on training data\n",
    "y_train_pred = mlp_model(X_train_tensor).argmax(dim=1).numpy()\n",
    "accuracy_train = accuracy_score(y_train_tensor.numpy(), y_train_pred)\n",
    "precision_train = precision_score(y_train_tensor.numpy(), y_train_pred)\n",
    "recall_train = recall_score(y_train_tensor.numpy(), y_train_pred)\n",
    "f1_train = f1_score(y_train_tensor.numpy(), y_train_pred)\n",
    "\n",
    "# Evaluate the model on testing data\n",
    "y_test_pred = mlp_model(X_val_tensor).argmax(dim=1).numpy()\n",
    "accuracy_test = accuracy_score(y_val_tensor.numpy(), y_test_pred)\n",
    "precision_test = precision_score(y_val_tensor.numpy(), y_test_pred)\n",
    "recall_test = recall_score(y_val_tensor.numpy(), y_test_pred)\n",
    "f1_test = f1_score(y_val_tensor.numpy(), y_test_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"Training Metrics For MLP:\")\n",
    "print(f\"Accuracy: {accuracy_train:.4f}\")\n",
    "print(f\"Precision: {precision_train:.4f}\")\n",
    "print(f\"Recall: {recall_train:.4f}\")\n",
    "print(f\"F1-Score: {f1_train:.4f}\")\n",
    "print(\"\\nTesting Metrics For MLP:\")\n",
    "print(f\"Accuracy: {accuracy_test:.4f}\")\n",
    "print(f\"Precision: {precision_test:.4f}\")\n",
    "print(f\"Recall: {recall_test:.4f}\")\n",
    "print(f\"F1-Score: {f1_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Neural Networks Average Values Google Word2Vec Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=avg_features_filtered_pretrained\n",
    "y=df_filtered['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 300)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/darshanrao/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Validation Loss: 0.0061, Accuracy: 82.96%\n",
      "Epoch: 2, Validation Loss: 0.0059, Accuracy: 83.64%\n",
      "Epoch: 3, Validation Loss: 0.0058, Accuracy: 83.66%\n",
      "Epoch: 4, Validation Loss: 0.0058, Accuracy: 83.50%\n",
      "Epoch: 5, Validation Loss: 0.0057, Accuracy: 84.24%\n",
      "Epoch: 6, Validation Loss: 0.0056, Accuracy: 84.35%\n",
      "Epoch: 7, Validation Loss: 0.0057, Accuracy: 84.30%\n",
      "Epoch: 8, Validation Loss: 0.0056, Accuracy: 84.14%\n",
      "Epoch: 9, Validation Loss: 0.0055, Accuracy: 84.54%\n",
      "Epoch: 10, Validation Loss: 0.0055, Accuracy: 84.57%\n",
      "Epoch: 11, Validation Loss: 0.0055, Accuracy: 84.64%\n",
      "Epoch: 12, Validation Loss: 0.0055, Accuracy: 84.43%\n",
      "Epoch: 13, Validation Loss: 0.0055, Accuracy: 84.53%\n",
      "Epoch: 14, Validation Loss: 0.0055, Accuracy: 84.55%\n",
      "Epoch: 15, Validation Loss: 0.0055, Accuracy: 84.54%\n",
      "Epoch: 16, Validation Loss: 0.0055, Accuracy: 84.54%\n",
      "Epoch: 17, Validation Loss: 0.0055, Accuracy: 84.53%\n",
      "Early stopping triggered. Training stopped.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "y_train_tensor = y_train_tensor - 1\n",
    "y_val_tensor = y_val_tensor - 1\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define the MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(300, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "mlp_model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mlp_model.parameters(), lr=0.001)\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, epochs=10, patience=3):\n",
    "    # Initialize early stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "    \n",
    "    # Scheduler for learning rate decay\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=patience // 2, factor=0.1, verbose=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad() # clears old gradients,\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target) # Calculate the pred-actual Loss\n",
    "            loss.backward() # Back proprogation (Calculating the gradient)\n",
    "            optimizer.step() # Weight updates\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval() # Switches the into evaluation mode\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad(): # Ensures gradient is not calculated(Saves memory and computation)\n",
    "            for data, target in val_loader:\n",
    "                output = model(data) ## Output in the form of probabitilities\n",
    "                val_loss += criterion(output, target).item() \n",
    "                pred = output.argmax(dim=1, keepdim=True) # Probabitlity with the max value is the output\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item() # counts the number of correct predictions\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        accuracy = 100. * correct / len(val_loader.dataset)\n",
    "        print(f'Epoch: {epoch+1}, Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "        \n",
    "        # Early stopping logic\n",
    "        if val_loss < best_val_loss: # if the  current loss is less the best loss\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0 \n",
    "        else: # no improvement in loss\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print('Early stopping triggered. Training stopped.')\n",
    "                early_stop = True\n",
    "                break\n",
    "        \n",
    "        # Learning rate scheduler step\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if early_stop:\n",
    "            print(\"Stopped early at epoch:\", epoch+1)\n",
    "            break\n",
    "\n",
    "\n",
    "train_model(mlp_model, criterion, optimizer, train_loader, val_loader, epochs=100, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Metrics For MLP:\n",
      "Accuracy: 0.8624\n",
      "Precision: 0.8586\n",
      "Recall: 0.8676\n",
      "F1-Score: 0.8631\n",
      "\n",
      "Testing Metrics For MLP:\n",
      "Accuracy: 0.8453\n",
      "Precision: 0.8416\n",
      "Recall: 0.8505\n",
      "F1-Score: 0.8460\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on training data\n",
    "y_train_pred = mlp_model(X_train_tensor).argmax(dim=1).numpy()\n",
    "accuracy_train = accuracy_score(y_train_tensor.numpy(), y_train_pred)\n",
    "precision_train = precision_score(y_train_tensor.numpy(), y_train_pred)\n",
    "recall_train = recall_score(y_train_tensor.numpy(), y_train_pred)\n",
    "f1_train = f1_score(y_train_tensor.numpy(), y_train_pred)\n",
    "\n",
    "# Evaluate the model on testing data\n",
    "y_test_pred = mlp_model(X_val_tensor).argmax(dim=1).numpy()\n",
    "accuracy_test = accuracy_score(y_val_tensor.numpy(), y_test_pred)\n",
    "precision_test = precision_score(y_val_tensor.numpy(), y_test_pred)\n",
    "recall_test = recall_score(y_val_tensor.numpy(), y_test_pred)\n",
    "f1_test = f1_score(y_val_tensor.numpy(), y_test_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"Training Metrics For MLP:\")\n",
    "print(f\"Accuracy: {accuracy_train:.4f}\")\n",
    "print(f\"Precision: {precision_train:.4f}\")\n",
    "print(f\"Recall: {recall_train:.4f}\")\n",
    "print(f\"F1-Score: {f1_train:.4f}\")\n",
    "print(\"\\nTesting Metrics For MLP:\")\n",
    "print(f\"Accuracy: {accuracy_test:.4f}\")\n",
    "print(f\"Precision: {precision_test:.4f}\")\n",
    "print(f\"Recall: {recall_test:.4f}\")\n",
    "print(f\"F1-Score: {f1_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Neural Networks Average Values Custom Word2Vec Ternary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=avg_features\n",
    "y=df['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((250000, 300), (250000,))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/darshanrao/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Validation Loss: 0.0114, Accuracy: 68.62%\n",
      "Epoch: 2, Validation Loss: 0.0111, Accuracy: 69.70%\n",
      "Epoch: 3, Validation Loss: 0.0110, Accuracy: 69.88%\n",
      "Epoch: 4, Validation Loss: 0.0111, Accuracy: 69.75%\n",
      "Epoch: 5, Validation Loss: 0.0110, Accuracy: 69.82%\n",
      "Epoch: 6, Validation Loss: 0.0109, Accuracy: 70.02%\n",
      "Epoch: 7, Validation Loss: 0.0110, Accuracy: 70.02%\n",
      "Epoch: 8, Validation Loss: 0.0109, Accuracy: 70.06%\n",
      "Epoch: 9, Validation Loss: 0.0109, Accuracy: 70.16%\n",
      "Epoch: 10, Validation Loss: 0.0109, Accuracy: 70.12%\n",
      "Epoch: 11, Validation Loss: 0.0109, Accuracy: 70.30%\n",
      "Epoch: 12, Validation Loss: 0.0109, Accuracy: 70.40%\n",
      "Epoch: 13, Validation Loss: 0.0110, Accuracy: 70.00%\n",
      "Epoch: 14, Validation Loss: 0.0109, Accuracy: 70.03%\n",
      "Epoch: 15, Validation Loss: 0.0110, Accuracy: 69.65%\n",
      "Epoch: 16, Validation Loss: 0.0109, Accuracy: 70.18%\n",
      "Epoch: 17, Validation Loss: 0.0109, Accuracy: 69.96%\n",
      "Epoch: 18, Validation Loss: 0.0109, Accuracy: 70.24%\n",
      "Epoch: 19, Validation Loss: 0.0108, Accuracy: 70.39%\n",
      "Epoch: 20, Validation Loss: 0.0108, Accuracy: 70.39%\n",
      "Epoch: 21, Validation Loss: 0.0108, Accuracy: 70.47%\n",
      "Epoch: 22, Validation Loss: 0.0108, Accuracy: 70.42%\n",
      "Epoch: 23, Validation Loss: 0.0109, Accuracy: 70.42%\n",
      "Epoch: 24, Validation Loss: 0.0109, Accuracy: 70.39%\n",
      "Epoch: 25, Validation Loss: 0.0109, Accuracy: 70.43%\n",
      "Epoch: 26, Validation Loss: 0.0109, Accuracy: 70.44%\n",
      "Epoch: 27, Validation Loss: 0.0109, Accuracy: 70.45%\n",
      "Epoch: 28, Validation Loss: 0.0109, Accuracy: 70.41%\n",
      "Epoch: 29, Validation Loss: 0.0109, Accuracy: 70.41%\n",
      "Early stopping triggered. Training stopped.\n"
     ]
    }
   ],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "y_train_tensor = y_train_tensor - 1\n",
    "y_val_tensor = y_val_tensor - 1\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define the MLP model\n",
    "class MLP_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_2, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(300, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "mlp_model_2 = MLP_2()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mlp_model_2.parameters(), lr=0.001)\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, epochs=10, patience=3):\n",
    "    # Initialize early stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "    \n",
    "    # Scheduler for learning rate decay\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=patience // 2, factor=0.1, verbose=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad() # clears old gradients,\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target) # Calculate the pred-actual Loss\n",
    "            loss.backward() # Back proprogation (Calculating the gradient)\n",
    "            optimizer.step() # Weight updates\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval() # Switches the into evaluation mode\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad(): # Ensures gradient is not calculated(Saves memory and computation)\n",
    "            for data, target in val_loader:\n",
    "                output = model(data) ## Output in the form of probabitilities\n",
    "                val_loss += criterion(output, target).item() \n",
    "                pred = output.argmax(dim=1, keepdim=True) # Probabitlity with the max value is the output\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item() # counts the number of correct predictions\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        accuracy = 100. * correct / len(val_loader.dataset)\n",
    "        print(f'Epoch: {epoch+1}, Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "        \n",
    "        # Early stopping logic\n",
    "        if val_loss < best_val_loss: # if the  current loss is less the best loss\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0 \n",
    "        else: # no improvement in loss\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print('Early stopping triggered. Training stopped.')\n",
    "                early_stop = True\n",
    "                break\n",
    "        \n",
    "        # Learning rate scheduler step\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if early_stop:\n",
    "            print(\"Stopped early at epoch:\", epoch+1)\n",
    "            break\n",
    "\n",
    "\n",
    "train_model(mlp_model_2, criterion, optimizer, train_loader, val_loader, epochs=100, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Metrics For MLP:\n",
      "Accuracy: 0.7287\n",
      "Precision: 0.7059\n",
      "Recall: 0.7287\n",
      "F1-Score: 0.7091\n",
      "\n",
      "Testing Metrics For MLP:\n",
      "Accuracy: 0.7041\n",
      "Precision: 0.6781\n",
      "Recall: 0.7041\n",
      "F1-Score: 0.6834\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on training data\n",
    "y_train_pred = mlp_model_2(X_train_tensor).argmax(dim=1).numpy()\n",
    "accuracy_train = accuracy_score(y_train_tensor.numpy(), y_train_pred)\n",
    "precision_train = precision_score(y_train_tensor.numpy(), y_train_pred, average='weighted')\n",
    "recall_train = recall_score(y_train_tensor.numpy(), y_train_pred, average='weighted')\n",
    "f1_train = f1_score(y_train_tensor.numpy(), y_train_pred, average='weighted')\n",
    "\n",
    "# Evaluate the model on testing data\n",
    "y_test_pred = mlp_model_2(X_val_tensor).argmax(dim=1).numpy()\n",
    "accuracy_test = accuracy_score(y_val_tensor.numpy(), y_test_pred)\n",
    "precision_test = precision_score(y_val_tensor.numpy(), y_test_pred, average='weighted')\n",
    "recall_test = recall_score(y_val_tensor.numpy(), y_test_pred, average='weighted')\n",
    "f1_test = f1_score(y_val_tensor.numpy(), y_test_pred, average='weighted')\n",
    "\n",
    "# Print the results\n",
    "print(\"Training Metrics For MLP:\")\n",
    "print(f\"Accuracy: {accuracy_train:.4f}\")\n",
    "print(f\"Precision: {precision_train:.4f}\")\n",
    "print(f\"Recall: {recall_train:.4f}\")\n",
    "print(f\"F1-Score: {f1_train:.4f}\")\n",
    "print(\"\\nTesting Metrics For MLP:\")\n",
    "print(f\"Accuracy: {accuracy_test:.4f}\")\n",
    "print(f\"Precision: {precision_test:.4f}\")\n",
    "print(f\"Recall: {recall_test:.4f}\")\n",
    "print(f\"F1-Score: {f1_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Neural Networks Average Values Google Word2Vec Ternary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=avg_features_pretrained\n",
    "y=df['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/darshanrao/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Validation Loss: 0.0113, Accuracy: 69.02%\n",
      "Epoch: 2, Validation Loss: 0.0111, Accuracy: 69.71%\n",
      "Epoch: 3, Validation Loss: 0.0111, Accuracy: 69.80%\n",
      "Epoch: 4, Validation Loss: 0.0111, Accuracy: 69.51%\n",
      "Epoch: 5, Validation Loss: 0.0110, Accuracy: 69.90%\n",
      "Epoch: 6, Validation Loss: 0.0109, Accuracy: 69.96%\n",
      "Epoch: 7, Validation Loss: 0.0109, Accuracy: 70.13%\n",
      "Epoch: 8, Validation Loss: 0.0109, Accuracy: 70.10%\n",
      "Epoch: 9, Validation Loss: 0.0109, Accuracy: 70.01%\n",
      "Epoch: 10, Validation Loss: 0.0109, Accuracy: 70.29%\n",
      "Epoch: 11, Validation Loss: 0.0109, Accuracy: 70.06%\n",
      "Epoch: 12, Validation Loss: 0.0109, Accuracy: 70.03%\n",
      "Epoch: 13, Validation Loss: 0.0109, Accuracy: 70.26%\n",
      "Epoch: 14, Validation Loss: 0.0109, Accuracy: 70.15%\n",
      "Epoch: 15, Validation Loss: 0.0109, Accuracy: 70.01%\n",
      "Epoch: 16, Validation Loss: 0.0109, Accuracy: 70.07%\n",
      "Epoch: 17, Validation Loss: 0.0108, Accuracy: 70.21%\n",
      "Epoch: 18, Validation Loss: 0.0108, Accuracy: 70.23%\n",
      "Epoch: 19, Validation Loss: 0.0109, Accuracy: 70.19%\n",
      "Epoch: 20, Validation Loss: 0.0109, Accuracy: 70.10%\n",
      "Epoch: 21, Validation Loss: 0.0109, Accuracy: 70.19%\n",
      "Epoch: 22, Validation Loss: 0.0109, Accuracy: 70.17%\n",
      "Epoch: 23, Validation Loss: 0.0109, Accuracy: 70.16%\n",
      "Epoch: 24, Validation Loss: 0.0109, Accuracy: 70.12%\n",
      "Epoch: 25, Validation Loss: 0.0109, Accuracy: 70.16%\n",
      "Epoch: 26, Validation Loss: 0.0109, Accuracy: 70.17%\n",
      "Epoch: 27, Validation Loss: 0.0109, Accuracy: 70.14%\n",
      "Epoch: 28, Validation Loss: 0.0109, Accuracy: 70.15%\n",
      "Early stopping triggered. Training stopped.\n"
     ]
    }
   ],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "y_train_tensor = y_train_tensor - 1\n",
    "y_val_tensor = y_val_tensor - 1\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define the MLP model\n",
    "class MLP_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_2, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(300, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "mlp_model_2 = MLP_2()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mlp_model_2.parameters(), lr=0.001)\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, epochs=10, patience=3):\n",
    "    # Initialize early stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "    \n",
    "    # Scheduler for learning rate decay\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=patience // 2, factor=0.1, verbose=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad() # clears old gradients,\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target) # Calculate the pred-actual Loss\n",
    "            loss.backward() # Back proprogation (Calculating the gradient)\n",
    "            optimizer.step() # Weight updates\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval() # Switches the into evaluation mode\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad(): # Ensures gradient is not calculated(Saves memory and computation)\n",
    "            for data, target in val_loader:\n",
    "                output = model(data) ## Output in the form of probabitilities\n",
    "                val_loss += criterion(output, target).item() \n",
    "                pred = output.argmax(dim=1, keepdim=True) # Probabitlity with the max value is the output\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item() # counts the number of correct predictions\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        accuracy = 100. * correct / len(val_loader.dataset)\n",
    "        print(f'Epoch: {epoch+1}, Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "        \n",
    "        # Early stopping logic\n",
    "        if val_loss < best_val_loss: # if the  current loss is less the best loss\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0 \n",
    "        else: # no improvement in loss\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print('Early stopping triggered. Training stopped.')\n",
    "                early_stop = True\n",
    "                break\n",
    "        \n",
    "        # Learning rate scheduler step\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if early_stop:\n",
    "            print(\"Stopped early at epoch:\", epoch+1)\n",
    "            break\n",
    "\n",
    "\n",
    "train_model(mlp_model_2, criterion, optimizer, train_loader, val_loader, epochs=100, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Metrics For MLP:\n",
      "Accuracy: 0.7270\n",
      "Precision: 0.7048\n",
      "Recall: 0.7270\n",
      "F1-Score: 0.7074\n",
      "\n",
      "Testing Metrics For MLP:\n",
      "Accuracy: 0.7015\n",
      "Precision: 0.6753\n",
      "Recall: 0.7015\n",
      "F1-Score: 0.6804\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on training data\n",
    "y_train_pred = mlp_model_2(X_train_tensor).argmax(dim=1).numpy()\n",
    "accuracy_train = accuracy_score(y_train_tensor.numpy(), y_train_pred)\n",
    "precision_train = precision_score(y_train_tensor.numpy(), y_train_pred, average='weighted')\n",
    "recall_train = recall_score(y_train_tensor.numpy(), y_train_pred, average='weighted')\n",
    "f1_train = f1_score(y_train_tensor.numpy(), y_train_pred, average='weighted')\n",
    "\n",
    "# Evaluate the model on testing data\n",
    "y_test_pred = mlp_model_2(X_val_tensor).argmax(dim=1).numpy()\n",
    "accuracy_test = accuracy_score(y_val_tensor.numpy(), y_test_pred)\n",
    "precision_test = precision_score(y_val_tensor.numpy(), y_test_pred, average='weighted')\n",
    "recall_test = recall_score(y_val_tensor.numpy(), y_test_pred, average='weighted')\n",
    "f1_test = f1_score(y_val_tensor.numpy(), y_test_pred, average='weighted')\n",
    "\n",
    "# Print the results\n",
    "print(\"Training Metrics For MLP:\")\n",
    "print(f\"Accuracy: {accuracy_train:.4f}\")\n",
    "print(f\"Precision: {precision_train:.4f}\")\n",
    "print(f\"Recall: {recall_train:.4f}\")\n",
    "print(f\"F1-Score: {f1_train:.4f}\")\n",
    "print(\"\\nTesting Metrics For MLP:\")\n",
    "print(f\"Accuracy: {accuracy_test:.4f}\")\n",
    "print(f\"Precision: {precision_test:.4f}\")\n",
    "print(f\"Recall: {recall_test:.4f}\")\n",
    "print(f\"F1-Score: {f1_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Neural Networks Concatenated Values Custom Word2Vec Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenated_word2vec(reviews, word2vec_model, vector_size, concat_size=10):\n",
    "    features = []\n",
    "\n",
    "    for review in reviews:\n",
    "        valid_words = [word for word in review if word in word2vec_model.wv.key_to_index]\n",
    "\n",
    "        if len(valid_words) >= concat_size:\n",
    "            # Take the embeddings of the first 'concat_size' valid words\n",
    "            word_vectors = np.array([word2vec_model.wv[word] for word in valid_words[:concat_size]])\n",
    "            concat_vector = word_vectors.flatten()\n",
    "        else:\n",
    "            # If there aren't enough valid words, pad the rest with zeros\n",
    "            word_vectors = np.array([word2vec_model.wv[word] for word in valid_words] +\n",
    "                                    [np.zeros(vector_size) for _ in range(concat_size - len(valid_words))])\n",
    "            concat_vector = word_vectors.flatten()\n",
    "\n",
    "        features.append(concat_vector)\n",
    "\n",
    "    return np.array(features)\n",
    "\n",
    "# Assuming 'tokenized_data' is your list of tokenized reviews and 'model' is your trained Word2Vec model\n",
    "concat_features = concatenated_word2vec(tokenized_data, model, vector_size=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 3000)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered DataFrame shape: (200000, 3)\n",
      "Filtered concat_features shape: (200000, 3000)\n"
     ]
    }
   ],
   "source": [
    "# Get the indices of the remaining rows after filtering\n",
    "filtered_indices = df_filtered.index.to_numpy()\n",
    "\n",
    "# Now, use these indices to filter the avg_features array\n",
    "concat_features_filtered = concat_features[filtered_indices]\n",
    "\n",
    "# Checking the dimensions to ensure they match\n",
    "print(\"Filtered DataFrame shape:\", df_filtered.shape)\n",
    "print(\"Filtered concat_features shape:\", concat_features_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=concat_features_filtered\n",
    "y=df_filtered['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200000, 3000), (200000,))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/darshanrao/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Validation Loss: 0.0069, Accuracy: 79.24%\n",
      "Epoch: 2, Validation Loss: 0.0068, Accuracy: 79.69%\n",
      "Epoch: 3, Validation Loss: 0.0070, Accuracy: 79.29%\n",
      "Epoch: 4, Validation Loss: 0.0071, Accuracy: 79.18%\n",
      "Epoch: 5, Validation Loss: 0.0076, Accuracy: 79.30%\n",
      "Early stopping triggered. Training stopped.\n"
     ]
    }
   ],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "y_train_tensor = y_train_tensor - 1\n",
    "y_val_tensor = y_val_tensor - 1\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define the MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(3000, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "mlp_model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mlp_model.parameters(), lr=0.001)\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, epochs=10, patience=3):\n",
    "    # Initialize early stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "    \n",
    "    # Scheduler for learning rate decay\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=patience // 2, factor=0.1, verbose=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad() # clears old gradients,\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target) # Calculate the pred-actual Loss\n",
    "            loss.backward() # Back proprogation (Calculating the gradient)\n",
    "            optimizer.step() # Weight updates\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval() # Switches the into evaluation mode\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad(): # Ensures gradient is not calculated(Saves memory and computation)\n",
    "            for data, target in val_loader:\n",
    "                output = model(data) ## Output in the form of probabitilities\n",
    "                val_loss += criterion(output, target).item() \n",
    "                pred = output.argmax(dim=1, keepdim=True) # Probabitlity with the max value is the output\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item() # counts the number of correct predictions\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        accuracy = 100. * correct / len(val_loader.dataset)\n",
    "        print(f'Epoch: {epoch+1}, Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "        \n",
    "        # Early stopping logic\n",
    "        if val_loss < best_val_loss: # if the  current loss is less the best loss\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0 \n",
    "        else: # no improvement in loss\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print('Early stopping triggered. Training stopped.')\n",
    "                early_stop = True\n",
    "                break\n",
    "        \n",
    "        # Learning rate scheduler step\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if early_stop:\n",
    "            print(\"Stopped early at epoch:\", epoch+1)\n",
    "            break\n",
    "\n",
    "\n",
    "train_model(mlp_model, criterion, optimizer, train_loader, val_loader, epochs=100, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Metrics For MLP:\n",
      "Accuracy: 0.8921\n",
      "Precision: 0.8866\n",
      "Recall: 0.8992\n",
      "F1-Score: 0.8929\n",
      "\n",
      "Testing Metrics For MLP:\n",
      "Accuracy: 0.7930\n",
      "Precision: 0.7876\n",
      "Recall: 0.8022\n",
      "F1-Score: 0.7948\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on training data\n",
    "y_train_pred = mlp_model(X_train_tensor).argmax(dim=1).numpy()\n",
    "accuracy_train = accuracy_score(y_train_tensor.numpy(), y_train_pred)\n",
    "precision_train = precision_score(y_train_tensor.numpy(), y_train_pred)\n",
    "recall_train = recall_score(y_train_tensor.numpy(), y_train_pred)\n",
    "f1_train = f1_score(y_train_tensor.numpy(), y_train_pred)\n",
    "\n",
    "# Evaluate the model on testing data\n",
    "y_test_pred = mlp_model(X_val_tensor).argmax(dim=1).numpy()\n",
    "accuracy_test = accuracy_score(y_val_tensor.numpy(), y_test_pred)\n",
    "precision_test = precision_score(y_val_tensor.numpy(), y_test_pred)\n",
    "recall_test = recall_score(y_val_tensor.numpy(), y_test_pred)\n",
    "f1_test = f1_score(y_val_tensor.numpy(), y_test_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"Training Metrics For MLP:\")\n",
    "print(f\"Accuracy: {accuracy_train:.4f}\")\n",
    "print(f\"Precision: {precision_train:.4f}\")\n",
    "print(f\"Recall: {recall_train:.4f}\")\n",
    "print(f\"F1-Score: {f1_train:.4f}\")\n",
    "print(\"\\nTesting Metrics For MLP:\")\n",
    "print(f\"Accuracy: {accuracy_test:.4f}\")\n",
    "print(f\"Precision: {precision_test:.4f}\")\n",
    "print(f\"Recall: {recall_test:.4f}\")\n",
    "print(f\"F1-Score: {f1_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Neural Networks Concatenated Values Google Word2Vec Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenated_word2vec_google(reviews,vector_size, concat_size=10):\n",
    "    features = []\n",
    "\n",
    "    for review in reviews:\n",
    "        valid_words = [word for word in review if word in wv.key_to_index]\n",
    "\n",
    "        if len(valid_words) >= concat_size:\n",
    "            # Take the embeddings of the first 'concat_size' valid words\n",
    "            word_vectors = np.array([wv[word] for word in valid_words[:concat_size]])\n",
    "            concat_vector = word_vectors.flatten()\n",
    "        else:\n",
    "            # If there aren't enough valid words, pad the rest with zeros\n",
    "            word_vectors = np.array([wv[word] for word in valid_words] +\n",
    "                                    [np.zeros(vector_size) for _ in range(concat_size - len(valid_words))])\n",
    "            concat_vector = word_vectors.flatten()\n",
    "\n",
    "        features.append(concat_vector)\n",
    "\n",
    "    return np.array(features)\n",
    "\n",
    "# Assuming 'tokenized_data' is your list of tokenized reviews and 'model' is your trained Word2Vec model\n",
    "concat_features_pretrained = concatenated_word2vec_google(tokenized_data,vector_size=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 3000)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_features_pretrained.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered DataFrame shape: (200000, 3)\n",
      "Filtered concat_features shape: (200000, 3000)\n"
     ]
    }
   ],
   "source": [
    "# Get the indices of the remaining rows after filtering\n",
    "filtered_indices = df_filtered.index.to_numpy()\n",
    "\n",
    "# Now, use these indices to filter the avg_features array\n",
    "concat_features_filtered_pretrained = concat_features_pretrained[filtered_indices]\n",
    "\n",
    "# Checking the dimensions to ensure they match\n",
    "print(\"Filtered DataFrame shape:\", df_filtered.shape)\n",
    "print(\"Filtered concat_features shape:\", concat_features_filtered_pretrained.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=concat_features_filtered_pretrained\n",
    "y=df_filtered['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200000, 3000), (200000,))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/darshanrao/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Validation Loss: 0.0072, Accuracy: 77.88%\n",
      "Epoch: 2, Validation Loss: 0.0071, Accuracy: 78.58%\n",
      "Epoch: 3, Validation Loss: 0.0072, Accuracy: 78.42%\n",
      "Epoch: 4, Validation Loss: 0.0075, Accuracy: 77.86%\n",
      "Epoch: 5, Validation Loss: 0.0081, Accuracy: 77.86%\n",
      "Early stopping triggered. Training stopped.\n"
     ]
    }
   ],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "y_train_tensor = y_train_tensor - 1\n",
    "y_val_tensor = y_val_tensor - 1\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define the MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(3000, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "mlp_model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mlp_model.parameters(), lr=0.001)\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, epochs=10, patience=3):\n",
    "    # Initialize early stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "    \n",
    "    # Scheduler for learning rate decay\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=patience // 2, factor=0.1, verbose=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad() # clears old gradients,\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target) # Calculate the pred-actual Loss\n",
    "            loss.backward() # Back proprogation (Calculating the gradient)\n",
    "            optimizer.step() # Weight updates\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval() # Switches the into evaluation mode\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad(): # Ensures gradient is not calculated(Saves memory and computation)\n",
    "            for data, target in val_loader:\n",
    "                output = model(data) ## Output in the form of probabitilities\n",
    "                val_loss += criterion(output, target).item() \n",
    "                pred = output.argmax(dim=1, keepdim=True) # Probabitlity with the max value is the output\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item() # counts the number of correct predictions\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        accuracy = 100. * correct / len(val_loader.dataset)\n",
    "        print(f'Epoch: {epoch+1}, Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "        \n",
    "        # Early stopping logic\n",
    "        if val_loss < best_val_loss: # if the  current loss is less the best loss\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0 \n",
    "        else: # no improvement in loss\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print('Early stopping triggered. Training stopped.')\n",
    "                early_stop = True\n",
    "                break\n",
    "        \n",
    "        # Learning rate scheduler step\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if early_stop:\n",
    "            print(\"Stopped early at epoch:\", epoch+1)\n",
    "            break\n",
    "\n",
    "\n",
    "train_model(mlp_model, criterion, optimizer, train_loader, val_loader, epochs=100, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Metrics For MLP:\n",
      "Accuracy: 0.8966\n",
      "Precision: 0.8920\n",
      "Recall: 0.9025\n",
      "F1-Score: 0.8972\n",
      "\n",
      "Testing Metrics For MLP:\n",
      "Accuracy: 0.7786\n",
      "Precision: 0.7738\n",
      "Recall: 0.7872\n",
      "F1-Score: 0.7804\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on training data\n",
    "y_train_pred = mlp_model(X_train_tensor).argmax(dim=1).numpy()\n",
    "accuracy_train = accuracy_score(y_train_tensor.numpy(), y_train_pred)\n",
    "precision_train = precision_score(y_train_tensor.numpy(), y_train_pred)\n",
    "recall_train = recall_score(y_train_tensor.numpy(), y_train_pred)\n",
    "f1_train = f1_score(y_train_tensor.numpy(), y_train_pred)\n",
    "\n",
    "# Evaluate the model on testing data\n",
    "y_test_pred = mlp_model(X_val_tensor).argmax(dim=1).numpy()\n",
    "accuracy_test = accuracy_score(y_val_tensor.numpy(), y_test_pred)\n",
    "precision_test = precision_score(y_val_tensor.numpy(), y_test_pred)\n",
    "recall_test = recall_score(y_val_tensor.numpy(), y_test_pred)\n",
    "f1_test = f1_score(y_val_tensor.numpy(), y_test_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"Training Metrics For MLP:\")\n",
    "print(f\"Accuracy: {accuracy_train:.4f}\")\n",
    "print(f\"Precision: {precision_train:.4f}\")\n",
    "print(f\"Recall: {recall_train:.4f}\")\n",
    "print(f\"F1-Score: {f1_train:.4f}\")\n",
    "print(\"\\nTesting Metrics For MLP:\")\n",
    "print(f\"Accuracy: {accuracy_test:.4f}\")\n",
    "print(f\"Precision: {precision_test:.4f}\")\n",
    "print(f\"Recall: {recall_test:.4f}\")\n",
    "print(f\"F1-Score: {f1_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Neural Networks Concatenated Values Custom Word2Vec Ternary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=concat_features\n",
    "y=df['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((250000, 3000), (250000,))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/darshanrao/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Validation Loss: 0.0128, Accuracy: 63.80%\n",
      "Epoch: 2, Validation Loss: 0.0127, Accuracy: 64.32%\n",
      "Epoch: 3, Validation Loss: 0.0128, Accuracy: 63.93%\n",
      "Epoch: 4, Validation Loss: 0.0130, Accuracy: 63.47%\n",
      "Epoch: 5, Validation Loss: 0.0132, Accuracy: 63.21%\n",
      "Epoch: 6, Validation Loss: 0.0136, Accuracy: 62.46%\n",
      "Epoch: 7, Validation Loss: 0.0141, Accuracy: 62.21%\n",
      "Epoch: 8, Validation Loss: 0.0146, Accuracy: 61.88%\n",
      "Epoch: 9, Validation Loss: 0.0157, Accuracy: 61.53%\n",
      "Epoch: 10, Validation Loss: 0.0164, Accuracy: 61.32%\n",
      "Epoch: 11, Validation Loss: 0.0168, Accuracy: 61.00%\n",
      "Epoch: 12, Validation Loss: 0.0173, Accuracy: 60.98%\n",
      "Early stopping triggered. Training stopped.\n"
     ]
    }
   ],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "y_train_tensor = y_train_tensor - 1\n",
    "y_val_tensor = y_val_tensor - 1\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define the MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(3000, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "mlp_model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mlp_model.parameters(), lr=0.001)\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, epochs=10, patience=3):\n",
    "    # Initialize early stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "    \n",
    "    # Scheduler for learning rate decay\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=patience // 2, factor=0.1, verbose=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad() # clears old gradients,\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target) # Calculate the pred-actual Loss\n",
    "            loss.backward() # Back proprogation (Calculating the gradient)\n",
    "            optimizer.step() # Weight updates\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval() # Switches the into evaluation mode\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad(): # Ensures gradient is not calculated(Saves memory and computation)\n",
    "            for data, target in val_loader:\n",
    "                output = model(data) ## Output in the form of probabitilities\n",
    "                val_loss += criterion(output, target).item() \n",
    "                pred = output.argmax(dim=1, keepdim=True) # Probabitlity with the max value is the output\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item() # counts the number of correct predictions\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        accuracy = 100. * correct / len(val_loader.dataset)\n",
    "        print(f'Epoch: {epoch+1}, Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "        \n",
    "        # Early stopping logic\n",
    "        if val_loss < best_val_loss: # if the  current loss is less the best loss\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0 \n",
    "        else: # no improvement in loss\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print('Early stopping triggered. Training stopped.')\n",
    "                early_stop = True\n",
    "                break\n",
    "        \n",
    "        # Learning rate scheduler step\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if early_stop:\n",
    "            print(\"Stopped early at epoch:\", epoch+1)\n",
    "            break\n",
    "\n",
    "\n",
    "train_model(mlp_model, criterion, optimizer, train_loader, val_loader, epochs=100, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Metrics For MLP:\n",
      "Accuracy: 0.8026\n",
      "Precision: 0.7943\n",
      "Recall: 0.8026\n",
      "F1-Score: 0.7926\n",
      "\n",
      "Testing Metrics For MLP:\n",
      "Accuracy: 0.6098\n",
      "Precision: 0.5892\n",
      "Recall: 0.6098\n",
      "F1-Score: 0.5961\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on training data\n",
    "y_train_pred = mlp_model(X_train_tensor).argmax(dim=1).numpy()\n",
    "accuracy_train = accuracy_score(y_train_tensor.numpy(), y_train_pred)\n",
    "precision_train = precision_score(y_train_tensor.numpy(), y_train_pred, average='weighted')\n",
    "recall_train = recall_score(y_train_tensor.numpy(), y_train_pred, average='weighted')\n",
    "f1_train = f1_score(y_train_tensor.numpy(), y_train_pred, average='weighted')\n",
    "\n",
    "# Evaluate the model on testing data\n",
    "y_test_pred = mlp_model(X_val_tensor).argmax(dim=1).numpy()\n",
    "accuracy_test = accuracy_score(y_val_tensor.numpy(), y_test_pred)\n",
    "precision_test = precision_score(y_val_tensor.numpy(), y_test_pred, average='weighted')\n",
    "recall_test = recall_score(y_val_tensor.numpy(), y_test_pred, average='weighted')\n",
    "f1_test = f1_score(y_val_tensor.numpy(), y_test_pred, average='weighted')\n",
    "\n",
    "# Print the results\n",
    "print(\"Training Metrics For MLP:\")\n",
    "print(f\"Accuracy: {accuracy_train:.4f}\")\n",
    "print(f\"Precision: {precision_train:.4f}\")\n",
    "print(f\"Recall: {recall_train:.4f}\")\n",
    "print(f\"F1-Score: {f1_train:.4f}\")\n",
    "print(\"\\nTesting Metrics For MLP:\")\n",
    "print(f\"Accuracy: {accuracy_test:.4f}\")\n",
    "print(f\"Precision: {precision_test:.4f}\")\n",
    "print(f\"Recall: {recall_test:.4f}\")\n",
    "print(f\"F1-Score: {f1_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Neural Networks Concatenated Values Google Word2Vec Ternary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=concat_features_pretrained\n",
    "y=df['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((250000, 3000), (250000,))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/darshanrao/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Validation Loss: 0.0131, Accuracy: 62.76%\n",
      "Epoch: 2, Validation Loss: 0.0130, Accuracy: 63.27%\n",
      "Epoch: 3, Validation Loss: 0.0131, Accuracy: 63.06%\n",
      "Epoch: 4, Validation Loss: 0.0132, Accuracy: 62.97%\n",
      "Epoch: 5, Validation Loss: 0.0136, Accuracy: 62.24%\n",
      "Epoch: 6, Validation Loss: 0.0141, Accuracy: 61.62%\n",
      "Epoch: 7, Validation Loss: 0.0146, Accuracy: 61.20%\n",
      "Epoch: 8, Validation Loss: 0.0154, Accuracy: 60.87%\n",
      "Epoch: 9, Validation Loss: 0.0163, Accuracy: 60.69%\n",
      "Epoch: 10, Validation Loss: 0.0169, Accuracy: 60.24%\n",
      "Epoch: 11, Validation Loss: 0.0174, Accuracy: 60.02%\n",
      "Epoch: 12, Validation Loss: 0.0178, Accuracy: 59.85%\n",
      "Early stopping triggered. Training stopped.\n"
     ]
    }
   ],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "y_train_tensor = y_train_tensor - 1\n",
    "y_val_tensor = y_val_tensor - 1\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define the MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(3000, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "mlp_model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mlp_model.parameters(), lr=0.001)\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, epochs=10, patience=3):\n",
    "    # Initialize early stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "    \n",
    "    # Scheduler for learning rate decay\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=patience // 2, factor=0.1, verbose=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad() # clears old gradients,\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target) # Calculate the pred-actual Loss\n",
    "            loss.backward() # Back proprogation (Calculating the gradient)\n",
    "            optimizer.step() # Weight updates\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval() # Switches the into evaluation mode\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad(): # Ensures gradient is not calculated(Saves memory and computation)\n",
    "            for data, target in val_loader:\n",
    "                output = model(data) ## Output in the form of probabitilities\n",
    "                val_loss += criterion(output, target).item() \n",
    "                pred = output.argmax(dim=1, keepdim=True) # Probabitlity with the max value is the output\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item() # counts the number of correct predictions\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        accuracy = 100. * correct / len(val_loader.dataset)\n",
    "        print(f'Epoch: {epoch+1}, Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "        \n",
    "        # Early stopping logic\n",
    "        if val_loss < best_val_loss: # if the  current loss is less the best loss\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0 \n",
    "        else: # no improvement in loss\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print('Early stopping triggered. Training stopped.')\n",
    "                early_stop = True\n",
    "                break\n",
    "        \n",
    "        # Learning rate scheduler step\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if early_stop:\n",
    "            print(\"Stopped early at epoch:\", epoch+1)\n",
    "            break\n",
    "\n",
    "\n",
    "train_model(mlp_model, criterion, optimizer, train_loader, val_loader, epochs=100, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Metrics For MLP:\n",
      "Accuracy: 0.8237\n",
      "Precision: 0.8166\n",
      "Recall: 0.8237\n",
      "F1-Score: 0.8156\n",
      "\n",
      "Testing Metrics For MLP:\n",
      "Accuracy: 0.5985\n",
      "Precision: 0.5797\n",
      "Recall: 0.5985\n",
      "F1-Score: 0.5863\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on training data\n",
    "y_train_pred = mlp_model(X_train_tensor).argmax(dim=1).numpy()\n",
    "accuracy_train = accuracy_score(y_train_tensor.numpy(), y_train_pred)\n",
    "precision_train = precision_score(y_train_tensor.numpy(), y_train_pred, average='weighted')\n",
    "recall_train = recall_score(y_train_tensor.numpy(), y_train_pred, average='weighted')\n",
    "f1_train = f1_score(y_train_tensor.numpy(), y_train_pred, average='weighted')\n",
    "\n",
    "# Evaluate the model on testing data\n",
    "y_test_pred = mlp_model(X_val_tensor).argmax(dim=1).numpy()\n",
    "accuracy_test = accuracy_score(y_val_tensor.numpy(), y_test_pred)\n",
    "precision_test = precision_score(y_val_tensor.numpy(), y_test_pred, average='weighted')\n",
    "recall_test = recall_score(y_val_tensor.numpy(), y_test_pred, average='weighted')\n",
    "f1_test = f1_score(y_val_tensor.numpy(), y_test_pred, average='weighted')\n",
    "\n",
    "# Print the results\n",
    "print(\"Training Metrics For MLP:\")\n",
    "print(f\"Accuracy: {accuracy_train:.4f}\")\n",
    "print(f\"Precision: {precision_train:.4f}\")\n",
    "print(f\"Recall: {recall_train:.4f}\")\n",
    "print(f\"F1-Score: {f1_train:.4f}\")\n",
    "print(\"\\nTesting Metrics For MLP:\")\n",
    "print(f\"Accuracy: {accuracy_test:.4f}\")\n",
    "print(f\"Precision: {precision_test:.4f}\")\n",
    "print(f\"Recall: {recall_test:.4f}\")\n",
    "print(f\"F1-Score: {f1_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network Padded Value Custom Word2Vec Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def padded_word2vec(reviews, word2vec_model, vector_size, pad_size=50):\n",
    "    features = []\n",
    "\n",
    "    for review in reviews:\n",
    "        valid_words = [word for word in review if word in word2vec_model.wv.key_to_index]\n",
    "        review_features = np.zeros((vector_size, pad_size), dtype=np.float32)\n",
    "\n",
    "        for i, word in enumerate(valid_words[:pad_size]):\n",
    "            review_features[:, i] = word2vec_model.wv[word]\n",
    "\n",
    "        features.append(review_features)\n",
    "\n",
    "    return np.array(features)\n",
    "\n",
    "# Assuming 'tokenized_data' is your list of tokenized reviews and 'model' is your trained Word2Vec model\n",
    "padded_features = padded_word2vec(tokenized_data, model, vector_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 300, 50)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows from the dataframe where 'class' is not equal to 3\n",
    "df = df[df['class'] != 3]\n",
    "\n",
    "# Get the indices of the remaining rows after filtering\n",
    "filtered_indices = df.index.to_numpy()\n",
    "\n",
    "# Now, use these indices to filter the avg_features array\n",
    "padded_features = padded_features[filtered_indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered DataFrame shape: (200000, 3)\n",
      "Filtered padded_features shape: (200000, 300, 50)\n"
     ]
    }
   ],
   "source": [
    "# Checking the dimensions to ensure they match\n",
    "print(\"Filtered DataFrame shape:\", df.shape)\n",
    "print(\"Filtered padded_features shape:\", padded_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=padded_features\n",
    "y=df['class']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((160000, 300, 50), (160000,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Define the CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=300, out_channels=50, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=50, out_channels=10, kernel_size=5, padding=2)\n",
    "        self.fc = None  # Will be initialized after the first forward pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool1d(x, kernel_size=2, stride=2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool1d(x, kernel_size=2, stride=2)\n",
    "        \n",
    "        # Check if the fc layer has been initialized, if not, do it dynamically\n",
    "        if self.fc is None:\n",
    "            # Calculate the correct input feature size\n",
    "            n_size = x.view(x.size(0), -1).size(1)\n",
    "            self.fc = nn.Linear(n_size, 2).to(x.device)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleCNN(num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN(\n",
      "  (conv1): Conv1d(300, 50, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (conv2): Conv1d(50, 10, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Print the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "\n",
    "y_train_tensor = y_train_tensor - 1\n",
    "y_test_tensor = y_test_tensor - 1\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 : Training loss: 0.3755 | Training Accuracy: 0.8373 | Val Accuracy: 0.8532\n",
      "Epoch 2/100 : Training loss: 0.3260 | Training Accuracy: 0.8627 | Val Accuracy: 0.8575\n",
      "Epoch 3/100 : Training loss: 0.2996 | Training Accuracy: 0.8741 | Val Accuracy: 0.8591\n",
      "Epoch 4/100 : Training loss: 0.2748 | Training Accuracy: 0.8866 | Val Accuracy: 0.8601\n",
      "Epoch 5/100 : Training loss: 0.2554 | Training Accuracy: 0.8949 | Val Accuracy: 0.8521\n",
      "Epoch 6/100 : Training loss: 0.2358 | Training Accuracy: 0.9040 | Val Accuracy: 0.8555\n",
      "Epoch 7/100 : Training loss: 0.2182 | Training Accuracy: 0.9120 | Val Accuracy: 0.8534\n",
      "Epoch 8/100 : Training loss: 0.2035 | Training Accuracy: 0.9179 | Val Accuracy: 0.8478\n",
      "Epoch 9/100 : Training loss: 0.1892 | Training Accuracy: 0.9242 | Val Accuracy: 0.8485\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(y_pred, y_true):\n",
    "    _, predicted = torch.max(y_pred.data, 1)\n",
    "    correct = (predicted == y_true).sum().item()\n",
    "    return correct / y_true.size(0)\n",
    "\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, epochs=10, patience=3):\n",
    "    best_val_acc = 0.0  # Track the best validation accuracy\n",
    "    patience_counter = 0  # Counter for how many epochs without improvement\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += calculate_accuracy(outputs, labels)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = running_corrects / len(train_loader)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_running_corrects = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                val_running_corrects += calculate_accuracy(outputs, labels)\n",
    "\n",
    "        val_epoch_acc = val_running_corrects / len(test_loader)\n",
    "        print(f'Epoch {epoch+1}/{epochs} : Training loss: {epoch_loss:.4f} | Training Accuracy: {epoch_acc:.4f} | Val Accuracy: {val_epoch_acc:.4f}')\n",
    "\n",
    "        # Early Stopping Check\n",
    "        if val_epoch_acc > best_val_acc:\n",
    "            best_val_acc = val_epoch_acc\n",
    "            patience_counter = 0  # Reset patience\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, test_loader, criterion, optimizer, epochs=100, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Metrics:\n",
      "Accuracy: 0.9358\n",
      "Precision: 0.9372\n",
      "Recall: 0.9358\n",
      "F1-Score: 0.9358\n",
      "\n",
      "Testing Metrics:\n",
      "Accuracy: 0.8485\n",
      "Precision: 0.8504\n",
      "Recall: 0.8485\n",
      "F1-Score: 0.8483\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_labels.extend(labels.tolist())\n",
    "    return all_preds, all_labels\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "y_train_pred, y_train_true = evaluate_model(model, train_loader)\n",
    "y_test_pred, y_test_true = evaluate_model(model, test_loader)\n",
    "\n",
    "accuracy_train, precision_train, recall_train, f1_train = calculate_metrics(y_train_true, y_train_pred)\n",
    "accuracy_test, precision_test, recall_test, f1_test = calculate_metrics(y_test_true, y_test_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"Training Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_train:.4f}\")\n",
    "print(f\"Precision: {precision_train:.4f}\")\n",
    "print(f\"Recall: {recall_train:.4f}\")\n",
    "print(f\"F1-Score: {f1_train:.4f}\")\n",
    "print(\"\\nTesting Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_test:.4f}\")\n",
    "print(f\"Precision: {precision_test:.4f}\")\n",
    "print(f\"Recall: {recall_test:.4f}\")\n",
    "print(f\"F1-Score: {f1_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network Padded Value Google Word2Vec Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def padded_word2vec_google(reviews,vector_size, pad_size=50):\n",
    "    features = []\n",
    "\n",
    "    for review in reviews:\n",
    "        valid_words = [word for word in review if word in wv.key_to_index]\n",
    "        review_features = np.zeros((vector_size, pad_size), dtype=np.float32)\n",
    "\n",
    "        for i, word in enumerate(valid_words[:pad_size]):\n",
    "            review_features[:, i] = wv[word]\n",
    "\n",
    "        features.append(review_features)\n",
    "\n",
    "    return np.array(features)\n",
    "\n",
    "# Assuming 'tokenized_data' is your list of tokenized reviews and 'model' is your trained Word2Vec model\n",
    "padded_features_pretrained = padded_word2vec_google(tokenized_data,vector_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 300, 50)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_features_pretrained.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows from the dataframe where 'class' is not equal to 3\n",
    "df = df[df['class'] != 3]\n",
    "\n",
    "# Get the indices of the remaining rows after filtering\n",
    "filtered_indices = df.index.to_numpy()\n",
    "\n",
    "# Now, use these indices to filter the avg_features array\n",
    "padded_features_pretrained = padded_features_pretrained[filtered_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered DataFrame shape: (200000, 3)\n",
      "Filtered padded_features shape: (200000, 300, 50)\n"
     ]
    }
   ],
   "source": [
    "# Checking the dimensions to ensure they match\n",
    "print(\"Filtered DataFrame shape:\", df.shape)\n",
    "print(\"Filtered padded_features shape:\", padded_features_pretrained.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=padded_features_pretrained\n",
    "y=df['class']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Define the CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=300, out_channels=50, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=50, out_channels=10, kernel_size=5, padding=2)\n",
    "        self.fc = None \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool1d(x, kernel_size=2, stride=2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool1d(x, kernel_size=2, stride=2)\n",
    "        \n",
    "        \n",
    "        if self.fc is None:\n",
    "            # Calculate the correct input feature size\n",
    "            n_size = x.view(x.size(0), -1).size(1)\n",
    "            self.fc = nn.Linear(n_size, 2).to(x.device)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleCNN(num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "\n",
    "y_train_tensor = y_train_tensor - 1\n",
    "y_test_tensor = y_test_tensor - 1\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 : Training loss: 0.4123 | Training Accuracy: 0.8165 | Val Accuracy: 0.8439\n",
      "Epoch 2/100 : Training loss: 0.3417 | Training Accuracy: 0.8554 | Val Accuracy: 0.8474\n",
      "Epoch 3/100 : Training loss: 0.3083 | Training Accuracy: 0.8719 | Val Accuracy: 0.8593\n",
      "Epoch 4/100 : Training loss: 0.2805 | Training Accuracy: 0.8851 | Val Accuracy: 0.8601\n",
      "Epoch 5/100 : Training loss: 0.2568 | Training Accuracy: 0.8959 | Val Accuracy: 0.8595\n",
      "Epoch 6/100 : Training loss: 0.2348 | Training Accuracy: 0.9062 | Val Accuracy: 0.8583\n",
      "Epoch 7/100 : Training loss: 0.2163 | Training Accuracy: 0.9155 | Val Accuracy: 0.8548\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(y_pred, y_true):\n",
    "    _, predicted = torch.max(y_pred.data, 1)\n",
    "    correct = (predicted == y_true).sum().item()\n",
    "    return correct / y_true.size(0)\n",
    "\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, epochs=10, patience=3):\n",
    "    best_val_acc = 0.0  \n",
    "    patience_counter = 0  \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += calculate_accuracy(outputs, labels)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = running_corrects / len(train_loader)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_running_corrects = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                val_running_corrects += calculate_accuracy(outputs, labels)\n",
    "\n",
    "        val_epoch_acc = val_running_corrects / len(test_loader)\n",
    "        print(f'Epoch {epoch+1}/{epochs} : Training loss: {epoch_loss:.4f} | Training Accuracy: {epoch_acc:.4f} | Val Accuracy: {val_epoch_acc:.4f}')\n",
    "\n",
    "        # Early Stopping Check\n",
    "        if val_epoch_acc > best_val_acc:\n",
    "            best_val_acc = val_epoch_acc\n",
    "            patience_counter = 0  # Reset patience\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, test_loader, criterion, optimizer, epochs=100, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Metrics:\n",
      "Accuracy: 0.9362\n",
      "Precision: 0.9362\n",
      "Recall: 0.9362\n",
      "F1-Score: 0.9362\n",
      "\n",
      "Testing Metrics:\n",
      "Accuracy: 0.8548\n",
      "Precision: 0.8548\n",
      "Recall: 0.8548\n",
      "F1-Score: 0.8547\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_labels.extend(labels.tolist())\n",
    "    return all_preds, all_labels\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "y_train_pred, y_train_true = evaluate_model(model, train_loader)\n",
    "y_test_pred, y_test_true = evaluate_model(model, test_loader)\n",
    "\n",
    "accuracy_train, precision_train, recall_train, f1_train = calculate_metrics(y_train_true, y_train_pred)\n",
    "accuracy_test, precision_test, recall_test, f1_test = calculate_metrics(y_test_true, y_test_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"Training Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_train:.4f}\")\n",
    "print(f\"Precision: {precision_train:.4f}\")\n",
    "print(f\"Recall: {recall_train:.4f}\")\n",
    "print(f\"F1-Score: {f1_train:.4f}\")\n",
    "print(\"\\nTesting Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_test:.4f}\")\n",
    "print(f\"Precision: {precision_test:.4f}\")\n",
    "print(f\"Recall: {recall_test:.4f}\")\n",
    "print(f\"F1-Score: {f1_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network Padded Value Custom Word2Vec Ternary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# Load model\n",
    "model = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def padded_word2vec(reviews, word2vec_model, vector_size, pad_size=50):\n",
    "    features = []\n",
    "\n",
    "    for review in reviews:\n",
    "        valid_words = [word for word in review if word in word2vec_model.wv.key_to_index]\n",
    "        review_features = np.zeros((vector_size, pad_size), dtype=np.float32)\n",
    "\n",
    "        for i, word in enumerate(valid_words[:pad_size]):\n",
    "            review_features[:, i] = word2vec_model.wv[word]\n",
    "\n",
    "        features.append(review_features)\n",
    "\n",
    "    return np.array(features)\n",
    "\n",
    "padded_features = padded_word2vec(tokenized_data, model, vector_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered DataFrame shape: (250000, 3)\n",
      "Filtered padded_features shape: (250000, 300, 50)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Filtered DataFrame shape:\", df.shape)\n",
    "print(\"Filtered padded_features shape:\", padded_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=padded_features\n",
    "y=df['class']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del padded_features\n",
    "del df\n",
    "del tokenized_data\n",
    "del model\n",
    "del X\n",
    "del y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200000, 300, 50), (200000,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 300, 50), (50000,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Define the CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=300, out_channels=50, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=50, out_channels=10, kernel_size=5, padding=2)\n",
    "        self.fc = None  # Will be initialized after the first forward pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool1d(x, kernel_size=2, stride=2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool1d(x, kernel_size=2, stride=2)\n",
    "        \n",
    "        \n",
    "        if self.fc is None:\n",
    "           \n",
    "            n_size = x.view(x.size(0), -1).size(1)\n",
    "            self.fc = nn.Linear(n_size, 3).to(x.device)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleCNN(num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "y_train_tensor = y_train_tensor - 1\n",
    "y_test_tensor = y_test_tensor - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train\n",
    "del X_test\n",
    "del y_train\n",
    "del y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_tensor\n",
    "del X_test_tensor\n",
    "del y_train_tensor\n",
    "del y_test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_dataset\n",
    "del test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 : Training loss: 0.8054 | Training Accuracy: 0.6555 | Val Accuracy: 0.6721\n",
      "Epoch 2/100 : Training loss: 0.7431 | Training Accuracy: 0.6866 | Val Accuracy: 0.6807\n",
      "Epoch 3/100 : Training loss: 0.7154 | Training Accuracy: 0.6985 | Val Accuracy: 0.6834\n",
      "Epoch 4/100 : Training loss: 0.6923 | Training Accuracy: 0.7100 | Val Accuracy: 0.6824\n",
      "Epoch 5/100 : Training loss: 0.6722 | Training Accuracy: 0.7187 | Val Accuracy: 0.6824\n",
      "Epoch 6/100 : Training loss: 0.6556 | Training Accuracy: 0.7259 | Val Accuracy: 0.6762\n",
      "Epoch 7/100 : Training loss: 0.6400 | Training Accuracy: 0.7335 | Val Accuracy: 0.6728\n",
      "Epoch 8/100 : Training loss: 0.6260 | Training Accuracy: 0.7408 | Val Accuracy: 0.6743\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(y_pred, y_true):\n",
    "    _, predicted = torch.max(y_pred.data, 1)\n",
    "    correct = (predicted == y_true).sum().item()\n",
    "    return correct / y_true.size(0)\n",
    "\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, epochs=10, patience=3):\n",
    "    best_val_acc = 0.0  # Track the best validation accuracy\n",
    "    patience_counter = 0  # Counter for how many epochs without improvement\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += calculate_accuracy(outputs, labels)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = running_corrects / len(train_loader)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_running_corrects = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                val_running_corrects += calculate_accuracy(outputs, labels)\n",
    "\n",
    "        val_epoch_acc = val_running_corrects / len(test_loader)\n",
    "        print(f'Epoch {epoch+1}/{epochs} : Training loss: {epoch_loss:.4f} | Training Accuracy: {epoch_acc:.4f} | Val Accuracy: {val_epoch_acc:.4f}')\n",
    "\n",
    "        # Early Stopping Check\n",
    "        if val_epoch_acc > best_val_acc:\n",
    "            best_val_acc = val_epoch_acc\n",
    "            patience_counter = 0  # Reset patience\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, test_loader, criterion, optimizer, epochs=100, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Metrics:\n",
      "Accuracy: 0.7621\n",
      "Precision: 0.7478\n",
      "Recall: 0.7621\n",
      "F1-Score: 0.7469\n",
      "\n",
      "Testing Metrics:\n",
      "Accuracy: 0.6744\n",
      "Precision: 0.6478\n",
      "Recall: 0.6744\n",
      "F1-Score: 0.6557\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_labels.extend(labels.tolist())\n",
    "    return all_preds, all_labels\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "y_train_pred, y_train_true = evaluate_model(model, train_loader)\n",
    "y_test_pred, y_test_true = evaluate_model(model, test_loader)\n",
    "\n",
    "accuracy_train, precision_train, recall_train, f1_train = calculate_metrics(y_train_true, y_train_pred)\n",
    "accuracy_test, precision_test, recall_test, f1_test = calculate_metrics(y_test_true, y_test_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"Training Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_train:.4f}\")\n",
    "print(f\"Precision: {precision_train:.4f}\")\n",
    "print(f\"Recall: {recall_train:.4f}\")\n",
    "print(f\"F1-Score: {f1_train:.4f}\")\n",
    "print(\"\\nTesting Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_test:.4f}\")\n",
    "print(f\"Precision: {precision_test:.4f}\")\n",
    "print(f\"Recall: {recall_test:.4f}\")\n",
    "print(f\"F1-Score: {f1_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network Padded Value Google Word2Vec Ternary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def padded_word2vec_google(reviews,vector_size, pad_size=50):\n",
    "    features = []\n",
    "\n",
    "    for review in reviews:\n",
    "        valid_words = [word for word in review if word in wv.key_to_index]\n",
    "        review_features = np.zeros((vector_size, pad_size), dtype=np.float32)\n",
    "\n",
    "        for i, word in enumerate(valid_words[:pad_size]):\n",
    "            review_features[:, i] = wv[word]\n",
    "\n",
    "        features.append(review_features)\n",
    "\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "padded_features_pretrained = padded_word2vec_google(tokenized_data,vector_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered DataFrame shape: (250000, 3)\n",
      "Filtered padded_features shape: (250000, 300, 50)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Filtered DataFrame shape:\", df.shape)\n",
    "print(\"Filtered padded_features shape:\", padded_features_pretrained.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=padded_features_pretrained\n",
    "y=df['class']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del padded_features_pretrained\n",
    "del df\n",
    "del tokenized_data\n",
    "del wv\n",
    "del X\n",
    "del y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200000, 300, 50), (200000,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 300, 50), (50000,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Define the CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=300, out_channels=50, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=50, out_channels=10, kernel_size=5, padding=2)\n",
    "        self.fc = None  # Will be initialized after the first forward pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool1d(x, kernel_size=2, stride=2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool1d(x, kernel_size=2, stride=2)\n",
    "        \n",
    "        \n",
    "        if self.fc is None:\n",
    "            # Calculate the correct input feature size\n",
    "            n_size = x.view(x.size(0), -1).size(1)\n",
    "            self.fc = nn.Linear(n_size, 3).to(x.device)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleCNN(num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "y_train_tensor = y_train_tensor - 1\n",
    "y_test_tensor = y_test_tensor - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train\n",
    "del X_test\n",
    "del y_train\n",
    "del y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_tensor\n",
    "del X_test_tensor\n",
    "del y_train_tensor\n",
    "del y_test_tensor\n",
    "del train_dataset\n",
    "del test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 : Training loss: 0.8213 | Training Accuracy: 0.6449 | Val Accuracy: 0.6752\n",
      "Epoch 2/100 : Training loss: 0.7363 | Training Accuracy: 0.6887 | Val Accuracy: 0.6865\n",
      "Epoch 3/100 : Training loss: 0.6987 | Training Accuracy: 0.7053 | Val Accuracy: 0.6843\n",
      "Epoch 4/100 : Training loss: 0.6710 | Training Accuracy: 0.7185 | Val Accuracy: 0.6891\n",
      "Epoch 5/100 : Training loss: 0.6478 | Training Accuracy: 0.7309 | Val Accuracy: 0.6878\n",
      "Epoch 6/100 : Training loss: 0.6270 | Training Accuracy: 0.7395 | Val Accuracy: 0.6888\n",
      "Epoch 7/100 : Training loss: 0.6089 | Training Accuracy: 0.7481 | Val Accuracy: 0.6804\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(y_pred, y_true):\n",
    "    _, predicted = torch.max(y_pred.data, 1)\n",
    "    correct = (predicted == y_true).sum().item()\n",
    "    return correct / y_true.size(0)\n",
    "\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, epochs=10, patience=3):\n",
    "    best_val_acc = 0.0  # Track the best validation accuracy\n",
    "    patience_counter = 0  # Counter for how many epochs without improvement\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += calculate_accuracy(outputs, labels)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = running_corrects / len(train_loader)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_running_corrects = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                val_running_corrects += calculate_accuracy(outputs, labels)\n",
    "\n",
    "        val_epoch_acc = val_running_corrects / len(test_loader)\n",
    "        print(f'Epoch {epoch+1}/{epochs} : Training loss: {epoch_loss:.4f} | Training Accuracy: {epoch_acc:.4f} | Val Accuracy: {val_epoch_acc:.4f}')\n",
    "\n",
    "        # Early Stopping Check\n",
    "        if val_epoch_acc > best_val_acc:\n",
    "            best_val_acc = val_epoch_acc\n",
    "            patience_counter = 0  # Reset patience\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, test_loader, criterion, optimizer, epochs=100, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Metrics:\n",
      "Accuracy: 0.7704\n",
      "Precision: 0.7598\n",
      "Recall: 0.7704\n",
      "F1-Score: 0.7622\n",
      "\n",
      "Testing Metrics:\n",
      "Accuracy: 0.6803\n",
      "Precision: 0.6635\n",
      "Recall: 0.6803\n",
      "F1-Score: 0.6697\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_labels.extend(labels.tolist())\n",
    "    return all_preds, all_labels\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "y_train_pred, y_train_true = evaluate_model(model, train_loader)\n",
    "y_test_pred, y_test_true = evaluate_model(model, test_loader)\n",
    "\n",
    "accuracy_train, precision_train, recall_train, f1_train = calculate_metrics(y_train_true, y_train_pred)\n",
    "accuracy_test, precision_test, recall_test, f1_test = calculate_metrics(y_test_true, y_test_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"Training Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_train:.4f}\")\n",
    "print(f\"Precision: {precision_train:.4f}\")\n",
    "print(f\"Recall: {recall_train:.4f}\")\n",
    "print(f\"F1-Score: {f1_train:.4f}\")\n",
    "print(\"\\nTesting Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_test:.4f}\")\n",
    "print(f\"Precision: {precision_test:.4f}\")\n",
    "print(f\"Recall: {recall_test:.4f}\")\n",
    "print(f\"F1-Score: {f1_test:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
